\documentclass[a4paper,10pt, notitlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage[portuguese]{babel}

%%%%%%%%%%%%%%%%%%%% Notation stuff
\newcommand{\pr}{\operatorname{Pr}} %% probability
\newcommand{\vr}{\operatorname{Var}} %% variance
\newcommand{\rs}{X_1, X_2, \ldots, X_n} %%  random sample
\newcommand{\irs}{X_1, X_2, \ldots} %% infinite random sample
\newcommand{\rsd}{x_1, x_2, \ldots, x_n} %%  random sample, realised
\newcommand{\bX}{\boldsymbol{X}} %%  random sample, contracted form (bold)
\newcommand{\bx}{\boldsymbol{x}} %%  random sample, realised, contracted form (bold)
\newcommand{\bT}{\boldsymbol{T}} %%  Statistic, vector form (bold)
\newcommand{\bt}{\boldsymbol{t}} %%  Statistic, realised, vector form (bold)
\newcommand{\emv}{\hat{\theta}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Title Page
\title{Segunda avaliação (A2)}
\author{Disciplina: Inferência Estatística \\ Professor: Luiz Max Carvalho}
\date{29 de Novembro de 2021}

\begin{document}
\maketitle

% \textbf{Data de Entrega: 19 de Agosto de 2020.}

\begin{center}
\fbox{\fbox{\parbox{1.0\textwidth}{\textsf{
    \begin{itemize}
        \item Por favor, entregue um único arquivo PDF;
        \item O tempo para realização da prova é de 3 horas, mais vinte minutos para upload do documento para o e-class;
        \item Leia a prova toda com calma antes de começar a responder;
        \item Responda todas as questões sucintamente;
        \item Marque a resposta final claramente com um quadrado, círculo ou figura geométrica de sua preferência;
        \item A prova vale 80 pontos. A pontuação restante é contada como bônus;
        \item Lembre-se de consultar o catálogo de fórmulas no fim deste documento.
    \end{itemize}}
}}}
\end{center}
\newpage
\section*{Dicas}
\begin{itemize}
 \item Se $Y \sim \operatorname{Poisson}(\lambda)$ então, para $\lambda$ grande o suficiente, temos
 \begin{itemize}
  \item $E[\sqrt{Y} + \sqrt{Y+1}] \approx \sqrt{4\lambda + 1}$;
  \item $\vr(\sqrt{Y} + \sqrt{Y+1}) \approx 1$. 
 \end{itemize} 
 Ademais, temos
 \begin{equation*}
 \frac{Y-\lambda}{\sqrt{\lambda}} \to \operatorname{Normal}(0, 1),
 \end{equation*}
 onde a seta representa convergência em distribuição.
 \item Se $X$ tem distribuição Poisson com média $\lambda$, então
 \[\pr\left(X \leq x\right) = Q(\floor*{x + 1}, \lambda), \]
 onde $$Q(x, s) = \frac{\Gamma(x, s)}{\Gamma(x)}$$ é a função Gama regularizada superior e $\floor*{y}$ é maior inteiro menor ou igual a $y$ -- também chamado de~\textit{floor}.
 Ademais, temos
\[ \frac{\partial}{\partial s} Q(x, s) = -\frac{e^{-s}s^{x-1}}{\Gamma(x)},\]
onde $\Gamma(x) = (x-1)!$ é a função Gamma.
\item  Se $\rs$ é uma amostra aleatória, então
\begin{equation*}
 \pr\left( \max(\rs) \leq m \right) = \prod_{i=1}^n \pr(X_i \leq m).
\end{equation*}
 \item Em uma regressão linear simples, temos:
  \begin{align*}
  \hat{\beta_0} &\sim \operatorname{Normal}\left(\beta_0, \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{s_x^2} \right) \right),\\
  \hat{\beta_1}  &\sim \operatorname{Normal}\left(\beta_1, \frac{\sigma^2}{s_x^2}\right),\\
  &\operatorname{Cov}\left(\hat{\beta_0}, \hat{\beta_1} \right)  = -\frac{\bar{x}\sigma^2}{s_x^2},
 \end{align*}
 onde $s_x = \sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}$ e $\hat{\beta_0}$ e $\hat{\beta_1}$ são os estimadores de máxima verossimilhança dos coeficientes.
 \end{itemize}
 
\newpage

\section*{1. A duck row}

Tio Patinhas e seus sobrinhos continuam sua jornada no aprendizado da Estatística e  consideram  $\rs \sim \operatorname{Uniforme}(0, \theta)$, $\theta > 0$, uma amostra aleatória com $M := \max(\rs)$.
Várias dúvidas e discordâncias surgiram entre os patinhos, no entanto. 
No que se segue, você deve utilizar seus conhecimentos de Estatística para esclarecer as coisas.


\begin{enumerate}[label=\alph*)]
\item (5 pontos) Huguinho diz que $Y = \theta^2 M$ é pivotal, Zezinho discorda.
Mostre que Huguinho está equivocado e encontre uma quantidade pivotal.
 \item (5 pontos) Mostre ao Pato Donald como utilizar a quantidade obtida no item anterior para obter um intervalo de confiança unilateral exato de $100\gamma \%$ para $\theta$. 
 \item (5 pontos) Defina\footnote{Preste atenção à formulação das hipóteses!} as hipóteses $H_0: \theta = 1/3$ e $H_1: \theta > 1/3$. 
Suponha que propomos um  procedimento de teste $\delta_c$ que rejeita $H_0$ quando $M > c$.
Deduza a função poder do teste e encontre $c$ de modo que o teste tenha tamanho $\alpha_0$.
 \item (10 pontos) Luisinho cismou que o  este teste tem razão de verossimilhanças monótona. 
 Ele está certo?
 Justifique sua resposta.
  \item (10 pontos) Tio Patinhas jura que este teste é uniformemente mais poderoso.
  Explique para Zezinho o que isso significa e depois discuta se o Tio Patinhas tem razão.
  Justifique sua resposta.
\end{enumerate}
\textcolor{red}{\textbf{Conceitos trabalhados}: Quantidade pivotal; intervalo de confiança; testes de hipótese; poder; tamanho; UMP.}\\ \textcolor{purple}{\textbf{Nível de dificuldade}: médio.}\\
\textcolor{blue}{
\textbf{Resolução:}
Para resolver a) vamos começar lembrando que
\begin{align*}
 \pr\left( M \leq m \right) &= \prod_{i=1}^n \pr(X_i \leq m),\\
 &= \left[\frac{m}{\theta}\right]^n.
\end{align*}
Desta forma, temos que 
\begin{align*}
 \pr\left( Y \leq y \right) &= \pr\left( M \leq \frac{y}{\theta^2} \right),\\
 &= \left[\frac{y}{\theta^3}\right]^n,
\end{align*}
donde concluímos que $Y = \theta^2 M$ não é pivotal.
Os cálculos acima sugerem que $W = M/\theta$ é de fato pivotal:
\begin{align*}
 \pr\left( W\leq w \right) &= \pr\left( M \leq \theta w \right),\\
 &= \left[\frac{\theta w}{\theta}\right]^n = \left[w\right]^n,
\end{align*}
mostrando que a distribuição de $W$ não depende de $\theta$, como desejado.
Para ajudar o Pato Donald em b), vamos primeiro notar que queremos um intervalo da forma $I(\bX) = (M, b(\bX))$ de modo que valha
\begin{equation*}
 \pr( M \leq \theta \leq b(\bX)) \geq \gamma.
\end{equation*}
Utilizando os cálculos acima, sabemos que
\begin{equation*}
 \pr\left(\theta \leq \frac{M}{w}\right) = 1 - w^n,
\end{equation*}
o que sugere que precisamos encontrar $w$ de modo que $1-w^n = \gamma$, ou seja, $w = (1-\gamma)^{1/n}$.
Assim, concluímos que $I(\bX) = (M, M/(1-\gamma)^{1/n})$ é um intervalo de confiança de $100\gamma \%$ exato para $\theta$.
Outras soluções são possíveis.
Para resolver c) precisamos lembrar que o poder $\pi(\theta \mid \delta_c)$ é a probabilidade de $M$ cair na região de rejeição:
\begin{align*}
 \pr(M > c \mid \theta) = 1 - \left[\frac{c}{\theta}\right]^n.
\end{align*}
Dessa forma, pela definição de tamanho de um teste, temos
\begin{align*}
 \alpha_0 := \pr(M > c \mid \theta = 1/3) = 1 - \left[3c\right]^n,
\end{align*}
portanto $c = \frac{(1-\alpha_0)^{1/n}}{3}$ é o valor crítico desejado.
Agora vamos atacar d): para começar sabemos que a função de verossimilhança é 
\begin{equation*}
 f_n(\bx \mid \theta) = \begin{cases}
 \frac{1}{\theta^n},\: M \leq \theta,\\
   0,\:\text{caso contrário}.
                         \end{cases}
\end{equation*}
Desta forma, queremos mostrar que para $\theta_1, \theta_2 \in (0, \infty)$ com $\theta_1 < \theta_2$ a razão $f_n(\bx \mid \theta_2)/ f_n(\bx \mid \theta_1)$ depende da amostra apenas através de $M$ e é função monotônica desta estatística.
De fato
\begin{align*}
 \frac{f_n(\bx \mid \theta_2)}{ f_n(\bx \mid \theta_1)} = \left[\frac{\theta_2}{\theta_1}\right]^n\mathbb{I}(M \leq \theta_1).
\end{align*}
Agora vamos descobrir se o teste proposto é UMP.
Vamos começar explicando para o Zezinho que um teste UMP $\delta$ é aquele que, para um tamanho $\alpha_0$, satisfaz 
\begin{equation*}
 \pi(\delta^\prime \mid \theta) \leq \pi(\delta \mid \theta),
\end{equation*}
para todo $\theta \in \Omega_1$ e para qualquer outro teste $\delta^\prime$.
Ou seja, um teste UMP~\textit{domina} todos os outros testes de tamanho $\alpha_0$ em termos de poder em $\Omega_1$ -- e, portanto, tem o menor erro do tipo II.
Segundo o Teorema 9.3.1 de DeGroot, provado também no item 3 do Trabalho IV, sabemos que para hipóteses da forma $H_0: \theta = \theta_0$  e $H_1 : \theta < \theta_0$ ou $H_1: \theta > \theta_0$ um teste com razão de verossimilhanças monotônica é UMP, confirmando a suspeita do Tio Patinhas.
$\blacksquare$
}

\section*{2. The God(damn) particle}

O bóson de Higgs\footnote{Assim nomeado em homenagem ao físico inglês Peter Higgs (1929-).}, que confere massa a partículas que de outra forma não teriam massa, foi a última partícula do chamado Modelo Padrão (\textit{standard model}) da Física Quântica a ter sua existência confirmada experimentalmente.

A ideia fundamental da descoberta experimental de novas partículas é a de comparar o que se espera sob um modelo base sem a nova partícula e um modelo proposto que a inclui.
Uma maneira de fazer isso é detectar o número $X$ de ocorrências (eventos) de decaimento de bósons e comparar esse número com o que é esperado sob o modelo mais simples.
Como o medidor é imperfeito, considere o seguinte modelo:
\begin{equation*}
 X \sim \operatorname{Poisson}(\beta  + \kappa\mu),
\end{equation*}
com $\mu >0$, onde vamos assumir que a taxa base $\beta > 0$ e a taxa esperada do bóson de Higgs $\kappa >0$ são conhecidas.
Desta forma, o modelo com $\mu = 0$ é o modelo de base e aquele onde $\mu =1$ é o modelo com o bóson de Higgs.

\begin{enumerate}[label=\alph*)]
 \item (10 pontos) Encontre o EMV de $\mu$ e comente sobre a sua adequação; o EMV sempre produz estimativas válidas? 
  \item (10 pontos) Encontre um intervalo de confiança bilateral de $100\gamma\%$ \textbf{aproximado} para $\mu$;
  
  \textit{Dica}: Você pode assumir que $\beta$ é grande o suficiente. Use as dicas no começo da prova.
  \item (10 pontos) Considere um teste em que assumimos que $X\sim\operatorname{Poisson}(\theta)$ e construímos um teste $\delta_c$ de modo que, se $X > c$, rejeitamos $H_0: \theta \leq \theta_0$.
  \begin{itemize}
  \item Mostre que o teste em questão é não-viesado.
   \item Escreva $\theta_0$ em função de $\beta$, $\kappa$ e $\mu$ de modo a testar um desvio em relação ao modelo base.
   
   \textit{Dica}: Existem várias respostas certas.   
  \end{itemize}
 \item (10 pontos) Formule um teste para $H_0: \mu =0$ vs $H_1: \mu > 0$ como um teste de razão de verossimilhanças;
\end{enumerate}
\textcolor{red}{\textbf{Conceitos trabalhados}: Estimador de máxima verossimilhança; intervalos de confiança aproximados; teste de razão de verossimilhanças.}\\
\textcolor{purple}{\textbf{Nível de dificuldade}: médio.}\\
\textcolor{blue}{
\textbf{Resolução:}
Para encontrar o EMV de $\mu$, vamos começar escrevendo a verossimilhança
\begin{equation*}
 f_n(X = x \mid \mu) = \frac{(\beta + \kappa\mu)^x \exp(-(\beta + \kappa\mu))}{x!},
\end{equation*}
e o seu logaritmo
\begin{equation*}
 \log (f_n(x \mid \mu)) =: l(\mu) = x\log(\beta + \kappa\mu)-(\beta + \kappa\mu)),
\end{equation*}
onde, por conveniência, descartamos termos que não dependem dos parâmetros.
Agora, temos
\begin{itemize}
 \item $l^\prime(\mu) = \frac{\kappa x}{\kappa\mu + \beta} - \kappa$,
 \item $l^{\prime\prime}(\mu) = - \frac{\kappa^2x}{(\beta + \kappa\mu)^2}$,
\end{itemize}
que imediatamente nos diz que a função é côncava com respeito a $\mu$ de modo que podemos obter
\begin{equation*}
 \hat{\mu}_{\text{EMV}} = \frac{x-\beta}{\kappa}
\end{equation*}
como nosso maximizador.
Note que esse estimador não garante estimativas em $(0, \infty)$; quando a contagem observada é, por acaso, menor que o esperado pelo modelo de base, isto é, menor que a taxa base $\beta$, temos estimativas negativas, que não fazem sentido.
Uma maneira de corrigir isso seria propor o estimador modificado
\begin{equation*}
 \hat{\mu}_{\text{ALT}} = \max\left(0, \frac{x-\beta}{\kappa}\right).
\end{equation*}
O interessante é notar que essa deterioração do estimador acontece perto do limite de detecção, isto é, quando a contagem é baixa e perto da taxa base, $\beta$.
Para obter um intervalo aproximado e responder b), vamos assumir que $\beta$ é grande o suficiente, de modo que $\beta + \kappa\mu$ se adequa às condições das dicas.
Em primeiro lugar, vamos definir $Z_{\gamma} := \Phi^{-1}((1+\gamma)/ 2)$, onde $\Phi$ é a CDF da Normal padrão.
Escrevendo $\sqrt{X} + \sqrt{X+1} =: U$, em seguida afirmamos que
\begin{align*}
 \gamma &\approx \pr\left(-Z_{\gamma} \leq U - \sqrt{4(\beta + \kappa\mu) + 1}\leq Z_{\gamma}\right),\\
 &= \pr\left(-Z_{\gamma} - U \leq  - \sqrt{4(\beta + \kappa\mu) + 1}\leq Z_{\gamma} - U \right),\\
 &=  \pr\left(Z_{\gamma} - U \leq \sqrt{4(\beta + \kappa\mu) + 1}\leq Z_{\gamma} + U \right),\\
   &= \pr\left(\frac{[Z_{\gamma} - U]^2 -1}{4} \leq (\beta + \kappa\mu) \leq \frac{[Z_{\gamma} + U]^2 - 1}{4}\right),\\
     &=  \pr\left(\frac{[Z_{\gamma} - U]^2 - (1 + 4\beta)}{4\kappa} \leq \mu \leq \frac{[Z_{\gamma} + U]^2 - (1 + 4\beta)}{4\kappa}\right).
\end{align*}
Nossa resposta final é portanto que
\begin{equation*}
 I(X) = \left(\frac{[Z_{\gamma} - U]^2 - (1 + 4\beta)}{4\kappa}, \frac{[Z_{\gamma} + U]^2 - (1 + 4\beta)}{4\kappa}\right)
\end{equation*}
é um intervalo bilateral aproximado de $100\gamma\%$ para $\mu$.
Para o item c) vamos lembrar que a probabilidade de rejeitar $H_0$ é:
\begin{equation*}
\pi(\theta \mid \delta_c) := \pr\left(X \geq c \mid \theta \right) = \sum_{k=c}^\infty \frac{e^{-\theta} \theta^k}{k!} = 1 - Q(\floor*{c + 1}, \theta). 
\end{equation*}
Utilizando a outra dica dada no começo da prova, vemos que  a derivada
\begin{align*}
 \frac{d\pi(\theta \mid \delta_c)}{d\theta} &= \frac{d}{d\theta} \left[1 - Q(\floor*{c + 1}, \theta)\right],\\
 &= -\left(-\frac{e^{-\theta}\theta^{\floor*{c+1}-1}}{\Gamma(\floor*{c + 1})}\right) = \frac{e^{-\theta}\theta^{c}}{\Gamma(\floor*{c + 1})}
\end{align*}
é não-negativa para todo $\theta \in (0, \infty)$, o que significa que a função poder do teste é monotônica não-decrescente em $\theta$.
Desse modo, concluímos que o teste é não-viesado: como a função poder é não-descrescente em $\theta$ temos que $\pi(\theta \mid \delta_c) \leq \pi(\theta^\prime \mid \delta_c)$ para todo par $(\theta, \theta^\prime)$ tal que $\theta \leq \theta_0$ e $\theta^\prime > \theta_0 \geq \theta$, isto é, $\theta \in \Omega_0$ e $\theta^\prime \in \Omega_1$.
Finalmente, para responder d), está claro que queremos $\theta_0 = \beta + \kappa\mu_0$ com $\mu_0 \geq 0$. 
Esta formulação dá conta de um modelo com o~\textit{background}, $\beta$, e mais alguma flutuação além dele.
Qualquer escolha de $\mu_0 \in [0,1)$ está tecnicamente certa, muito embora algumas sejam mais úteis científicamente que outras.
Uma possível é $\mu_0 = 1/2$~\citep{VanDyk2014}.
Agora vamos reponder à questão d) nos aproveitando do que já foi feito no item a):
\begin{align*}
 r_{01}(x) &= \frac{\sup_{\mu \in \Omega_0}f_n(x \mid \mu)}{\sup_{\mu \in \Omega_1}f_n(x \mid \mu)},\\
 &= \frac{f_n(x \mid \mu = 0)}{f_n(x \mid \mu = \hat{\mu}_{\text{EMV}})},\\
 &= \frac{\beta^x\exp(-\beta)}{x^x\exp(-x)}.
\end{align*}
Agora podemos formular um teste de razão de verossimilhanças $\delta_k$ como
\begin{equation*}
 \delta_k = \begin{cases}
             \text{Rejeitar}\: H_0\:, \text{se}\: r_{01}(x) < k,\\
             \text{Falhar em rejeitar}\: H_0\:, \text{se}\: r_{01}(x) \geq k.
            \end{cases}
\end{equation*}
\paragraph{Comentários adicionais} Esta questão foi baseada no artigo de~\cite{VanDyk2014} sobre a descoberta do bóson de Higgs e no trabalho de~\cite{Freeman1950} sobre transformações estabilizadoras da variância para o caso Poisson.
$\blacksquare$
}

\section*{3. Gosto muito de você, linearzinho.}

O modelo linear (de regressão) é um dos cavalos de batalha da Estatística, sendo aplicado em problemas de Finanças, Medicina e Engenharia.
Vamos agora estudar como utilizar as propriedades deste modelo para desenhar experimentos com garantias matemáticas de desempenho e obter estimadores de quantidades de interesse.

\begin{enumerate}[label=\alph*)]
 \item (5 pontos) Uma prática comum em regressão é a de \textbf{centrar} a variável independente (covariável), isto é subtrair a média; isto facilita a interpretação do intercepto e também simplifica alguns cálculos importantes.
 Mostre que no caso com a covariável centrada, $\hat{\beta_0}$ e $\hat{\beta_1}$ são independentes;
 \item (7,5 pontos) Mais uma vez considerando o caso centrado, mostre 
 como obter o número de observações $n$ que faz com que a variância do estimador de máxima verossimilhança do intercepto seja menor que $v > 0$;
 \item (5 pontos) Mostre como obter um estimador não-viesado da quantidade $\theta = a\beta_0 + b\beta_1 + c$, com $a, b, c \neq 0$, e encontre o seu erro quadrático médio.
 \item (7,5 pontos)  Quando $x_{\text{pred}} = \bar{x}$, mostre como obter  o número de observações $n$ necessário para que o intervalo de predição de $100(1-\alpha_0)\%$ para a variável-resposta ($Y$) tenha largura menor ou igual a $l>0$ com probabilidade pelo menos $\gamma$.
 
 \textit{Dicas}:(i) A expressão dependerá~\textit{também} da variância dos resíduos, $\sigma^2$ e (ii) Você não precisa calcular $n$, apenas mostrar o procedimento para obtê-lo.
 
 \end{enumerate}
\textcolor{red}{\textbf{Conceitos trabalhados}: Regressão linear; desenho experimental; quantidades derivadas.}\\ \textcolor{purple}{\textbf{Nível de dificuldade}: médio.}\\
\textcolor{blue}{
\textbf{Resolução:}
Para resolver a) vamos precisar apenas olhar para as dicas e perceber que quando substituímos a covariável original $X$ por $X^\prime = X-\bar{x}$ temos $\bar{x}^\prime = 0$ e portanto $\operatorname{Cov}\left(\hat{\beta_0}, \hat{\beta_1} \right)  = -\frac{\bar{x}^\prime \sigma^2}{s_x^2} = 0$.
Para afirmarmos que $\hat{\beta_0}$ e $\hat{\beta_1}$ são independentes é  preciso lembrar que estes estimadores têm distribuição conjunta Normal bivariada; quando a covariância é zero, sabemos que são independentes.
A resposta de b) vem mais uma vez utilizando a dica dada.
Vemos que no caso centrado a variância de $\hat{\beta_0}$ é $\sigma^2/n$. 
Desta forma, precisamos apenas encontrar $n$ tal que $\sigma^2/n < v$, isto é $n > \sigma^2/v$.
Como sabemos que os estimadores dos coeficientes são não-viesados (trabalhado em aula, presente nas dicas), podemos encontrar $\hat{\theta} = a\hat{\beta_0} + b\hat{\beta_1} +c$ como nosso estimador não-viesado de $\theta$.
O EQM de tal estimador é a sua variância:
\begin{align*}
 E[(\hat{\theta}-\theta)^2] &= \vr(\hat{\theta}) = a^2 \vr(\hat{\beta_0}) + b^2\vr(\hat{\beta_1}) -ab \operatorname{Cov}(\hat{\beta_0}, \hat{\beta_1}),\\
 &=  a^2 \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{s_x^2} \right) + b^2\frac{\sigma^2}{s_x^2} + ab \frac{\bar{x}\sigma^2}{s_x^2},\\
 &= \sigma^2 \left(\frac{a^2}{n} + \frac{a^2\bar{x}^2}{s_x^2} + \frac{b^2}{s_x^2} + \frac{ab\bar{x}}{s_x^2}\right).
\end{align*}
Por fim, vamos responder d).
Note que a expressão necessária aqui é a do intervalo de predição:
\begin{equation*}
 \hat{Y} \pm  c(n, \alpha_0)\cdot\hat{\sigma}_r^\prime \cdot \sqrt{\left[ 1+ \frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2} \right]},
\end{equation*}
onde
\begin{equation*}
 c(n, \alpha_0) := T^{-1}\left(1-\frac{\alpha_0}{2}; n-2\right),
\end{equation*}
e
\begin{equation*}
 \hat{\sigma}_r^\prime := \sqrt{\frac{\sum_{i=1}^n \left(Y_i - \hat{\beta_0} - \hat{\beta_1}x_i \right)^2}{n-2}}.
\end{equation*}
Quando $x_{\text{pred}} = \bar{x}$ a expressão se reduz um pouco e podemos deduzir que a largura do intervalo é
\begin{equation*}
 \hat{l} = 2 \cdot c(n, \alpha_0) \cdot \hat{\sigma}_r^\prime \sqrt{\left[ 1+ \frac{1}{n}\right]}.
\end{equation*}
Desejamos, portanto, encontrar $n$ tal que
\begin{align*}
 \pr\left(\hat{l} < l\right) &\geq \gamma,\\
 \pr\left( \hat{\sigma}_r^\prime < \frac{l}{2 \cdot c(n, \alpha_0) \cdot \sqrt{\left[ 1+ \frac{1}{n}\right]} }\right) &\geq \gamma,\\
\end{align*}
isto é conseguimos reduzir nossa afirmação probabilística a uma afirmação com respeito à f.d.a. (ou CDF) de $\hat{\sigma}_r^\prime$.
Para completar nossos cálculos só precisamos nos lembrar que $n  \hat{\sigma}_r^\prime/\sigma^2$ tem distribuição qui-quadrado com $n-2$ graus de liberdade (De Groot, Teorema 11.3.2) e, portanto,
\begin{equation*}
 \pr\left(\hat{\sigma}_r^\prime \leq a \right) = F_\chi\left(\frac{\sigma^2}{n}a; n- 2\right).
\end{equation*}
}
\newpage
\section*{Fórmulas úteis}
\textbf{Como usar este catálogo:} as fórmulas dadas aqui estão propositalmente privadas do seu contexto.
O objetivo desta coleção é ajudar você a lembrar das expressões.
Entretanto, saber quais expressões são utilizadas em que contexto é sua tarefa.
\begin{itemize}
 \item $ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$;
 \item $\hat{\sigma}^\prime = \sqrt{\frac{1}{n-1}\sum_{i=1}^n \left(X_i - \bar{X}_n\right)^2}$;
 \item $S_X^2 = \sum_{i=1}^m (X_i-\bar{X}_m)^2$;
 \item $S_Y^2 = \sum_{j=1}^n (Y_j-\bar{Y}_n)^2$;
 \item $U = \frac{\sqrt{m + n - 2}(\bar{X}_m - \bar{Y}_n)}{\sqrt{\left(\frac{1}{m} + \frac{1}{n}\right) (S_X^2 + S_Y^2)}}$;
 \item $ V = \frac{S_X^2/(m-1)}{S_Y^2/(n-1)}$;
 \item $\bar{x} = (1/n)\sum_{i=1}^n X_i$;
 \item $\bar{y} = (1/n)\sum_{i=1}^n Y_i$;
\item \begin{equation*}
        E\left[\left(\hat{Y} - Y\right)^2\right] = \sigma^2 \left(1 + \frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2}\right).
      \end{equation*}
  \item \begin{equation*}
 \hat{\sigma}_r^\prime := \sqrt{\frac{\sum_{i=1}^n \left(Y_i - \hat{\beta_0} - \hat{\beta_1}x_i \right)^2}{n-2}}.
\end{equation*}     
\item \begin{align*}
 &\hat{\beta_0} \pm \hat{\sigma}^\prime c\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}}\quad \text{e}\quad \hat{\beta_1} \pm c\frac{\hat{\sigma}^\prime}{s_x},\\
 &\hat{\beta_0} + \hat{\beta_1}x_{\text{pred}} \pm c \hat{\sigma}^\prime \sqrt{\frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2} },
\end{align*}    
\item \begin{equation*}
 \hat{Y} \pm c\hat{\sigma}_r^\prime \sqrt{\left[ 1+ \frac{1}{n} + \frac{\left(x_{\text{pred}}-\bar{x}\right)^2}{s_x^2} \right]},
\end{equation*}
onde $c = T^{-1}(1-\frac{\alpha_0}{2}; n-2)$.
\end{itemize}


\bibliographystyle{apalike}
\bibliography{a2_2021}

\end{document}          
